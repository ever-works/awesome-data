# Awesome VLM Architectures

**Description**  
A curated “awesome list” of Vision-Language Model (VLM) architectures and related research resources. It focuses on multimodal models that jointly process images and text, highlighting their architectures, training procedures, and datasets.

**Category:** Themed Directories  
**Tags:** AI, computer vision, deep learning

---

## Overview

Awesome VLM Architectures is a GitHub repository that aggregates:
- Well-known Vision-Language Models (VLMs)
- Architectural breakdowns of these models
- Notes on training procedures
- Information about datasets used to train them
- Research resources and references

The list centers on models capable of tasks such as:
- Visual Question Answering (VQA)
- Image captioning
- Text-to-image search and retrieval

---

## Features

- **Curated VLM Model Catalog**  
  - Collection of famous Vision-Language Models.  
  - Includes entries such as LLaVA, LLaVA 1.5, LLaVA 1.6, PaliGemma, PaliGemma 2, and others (via the README model list).

- **Architecture-Focused Summaries**  
  - Describes the multimodal architectures used by VLMs.  
  - Emphasizes how models fuse visual and textual information, including:  
    - Multimodal fusion mechanisms  
    - Cross-attention between image and text streams  
    - Image-text matching strategies  
    - Masked language modeling components (when applicable).

- **Task-Oriented Model Descriptions**  
  - Highlights the core capabilities of VLMs, including:  
    - Visual Question Answering (VQA)  
    - Image caption generation  
    - Text-to-image and image-to-text retrieval/search.

- **Training Procedure Notes**  
  - For each architecture, the repository aims to include:  
    - High-level training setup and objectives.  
    - How visual and textual encoders are trained or fine-tuned.  
    - Use of instruction tuning (e.g., visual instruction tuning for models like LLaVA).

- **Dataset References**  
  - Documents the major datasets used to train or evaluate each VLM.  
  - Helps users trace which datasets underlie reported capabilities and benchmarks.

- **Expandable Per-Architecture Details**  
  - The README indicates that each architecture entry can be expanded for further details, allowing deeper inspection of:  
    - Model structure  
    - Training scheme  
    - Data sources  
    - Relevant research papers.

- **Research-Oriented Resource List**  
  - Designed as a reference hub for:  
    - Researchers studying multimodal learning.  
    - Practitioners comparing VLM approaches.  
    - Students looking for an entry point into VLM architectures.

- **Community & Governance Files**  
  - `CODE-OF-CONDUCT.md` and `CONTRIBUTING.md` included for community contributions.  
  - Standard project metadata: `.gitattributes`, `.gitignore`, `LICENSE`.

---

## Use Cases

- Explore and compare well-known VLM architectures.  
- Identify which models support tasks like VQA, captioning, and text-image search.  
- Discover training procedures and datasets used in leading multimodal models.  
- Use as a reading list or roadmap for learning about VLM research.

---

## License

- A `LICENSE` file is present in the repository; users should refer to it directly on GitHub for exact terms.

---

## Pricing

- Not applicable. This is an open GitHub “awesome list” repository, not a commercial product with pricing plans.